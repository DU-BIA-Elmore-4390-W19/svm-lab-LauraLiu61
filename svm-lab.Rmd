---
title: "SVM_Lab"
author: "Yao Liu"
date: "3/5/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. This question refers to Chapter 9 Problem 8 beginning on page 371 in the 
text. 

    a. Create a training sample that has roughly 80% of the observations. Use
  `set.seed(19823)`.
```{r libs, message = F, warning = F, include = F}
library(MASS)
library(tidyverse)
library(broom)
library(caret)     #look for ROC curve
library(ISLR)
library(janitor)
library(plotROC)
library(doMC)
library(kernlab)   #kernel method for SVM
theme_set(theme_bw())
```

```{r}
df <- tbl_df(OJ)
attach(df)
set.seed(19823)
inTraining <- createDataPartition(Purchase, p = .5, list = F )
training <- df[inTraining, ]
testing  <- df[-inTraining, ]
```
  
    b. Use the `kernlab` package to fit a support vector classifier to the 
  training data using `C = 0.01`. 
```{r}
purchase_svm_m1 <- ksvm(Purchase ~ ., data = training,
                     type = "C-svc", kernel = 'vanilladot', 
                     C = 0.01, prob.model = T)  
```
  
    c. Compute the confusion matrix for the training data. Report the overall 
  error rates, sensitivity, and specificity. 
```{r}
confusionMatrix(table(predict(purchase_svm_m1, newdata = testing),testing$Purchase),positive = "MM")
```
  
    d. Construct the ROC curve. 
```{r}
fits_svc <- predict(purchase_svm_m1, newdata = training, type = "probabilities")
new_fits <- mutate(training, 
                   svc_probs = fits_svc[,2],
                   default = if_else(Purchase == "CH", 0, 1))          
p <- ggplot(data = new_fits,
            aes(d = default , m = svc_probs))
p + geom_roc(n.cuts = 0, col = "orange") +
  style_roc()
```
    
    e. Use the `train` function from the `caret` package to find an optimal cost
  parameter (`C`) in the range 0.01 to 10. Use `seq(0.01, 10, len = 20)`. 
```{r}
fit_control <- trainControl(method = "repeatedcv",
                           number = 10, 
                           repeats = 3)
purchase_train <- train(Purchase ~ .,
                     data = training,
                     method = "svmLinear",     #apply hyperplane here 
                     trControl = fit_control,
                     tuneGrid = data.frame(C = seq(0.01,10,len  = 20))) #add the buffer area here. 10 means the larger area of buffer 
purchase_train 
```
  
    f. Compute the training and test classification error.
```{r}
confusionMatrix(table(predict(purchase_train , newdata = testing), 
                      testing$Purchase), positive = "MM")
```
    
    g. Repeat (b) - (d) using an SVM with a polynomial kernel with degree 2. 
```{r}
purchase_ply <- ksvm(Purchase ~ ., 
               data = training,
               type = "C-svc", kernel = 'polydot', 
               kpar = list(degree = 2, scale = .1),   #a polynomial kernel with degree 2
               C = .01, prob.model = T)

fits_svm <- predict(purchase_ply, newdata = training, type = "probabilities")
svm_pred <- mutate(new_fits, svm_probs = fits_svm[, 2])
p <- ggplot(data = svm_pred,
            aes(d = default, m = svm_probs))
p + geom_roc(n.cuts = 0, col = "pink") +
  style_roc()

```
    
```{r}
confusionMatrix(table(predict(purchase_ply, newdata = testing), 
                      testing$Purchase), positive = "MM")
```
  
    i. Repeat (b) - (d) using an SVM with a radial basis kernel. Train it. 
```{r}
purchase_svm_rad <- ksvm(Purchase ~ ., data = training,
                   type = "C-svc", kernel = 'rbfdot', 
                   kpar = list(sigma = .1), 
                   prob.model = T)
fits_svm_rad <- predict(purchase_svm_rad, newdata = training, type = "probabilities")

svm_pred_rad <- mutate(svm_pred, svm_probs_rad = fits_svm_rad[, 2])
p <- ggplot(data = svm_pred_rad,
            aes(d = default, m = svm_probs_rad))
p + geom_roc(n.cuts = 0) +
  style_roc() 

```
```{r}
confusionMatrix(table(predict(purchase_svm_rad, newdata = testing), 
                      testing$Purchase), positive = "MM")

```


2. Train one of the SVM models using a single core, 2 cores, and 4 cores.
Compare the speedup (if any). 

```{r}
registerDoMC(cores = 1)  # single core
svm_cor1 <- system.time(ksvm(Purchase ~ ., 
                         data = training,
                         type = "C-svc", kernel = 'polydot', 
                         kpar = list(degree = 2, scale = .1), 
                         C = .01, prob.model = T))
svm_cor1

```
```{r}
registerDoMC(cores = 2)  # 2 cores
svm_cor2 <- system.time(ksvm(Purchase ~ ., 
                         data = training,
                         type = "C-svc", kernel = 'polydot', 
                         kpar = list(degree = 2, scale = .1), 
                         C = .01, prob.model = T))
svm_cor2

```

```{r}
registerDoMC(cores = 4)  # 4 cores
svm_cor4 <- system.time(ksvm(Purchase ~ ., 
                         data = training,
                         type = "C-svc", kernel = 'polydot', 
                         kpar = list(degree = 2, scale = .1), 
                         C = .01, prob.model = T))
svm_cor4

```
3. You might want to look at `rbenchmark` or `microbenchmark` packages for 
timing.
